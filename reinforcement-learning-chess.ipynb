{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Chess package with Chess Game Utilities","metadata":{}},{"cell_type":"code","source":"!pip install chess","metadata":{"execution":{"iopub.status.busy":"2023-10-15T06:35:13.708246Z","iopub.execute_input":"2023-10-15T06:35:13.708878Z","iopub.status.idle":"2023-10-15T06:35:23.294556Z","shell.execute_reply.started":"2023-10-15T06:35:13.708846Z","shell.execute_reply":"2023-10-15T06:35:23.293398Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting chess\n  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: chess\nSuccessfully installed chess-1.10.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Defining Agents to play the chess game","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, Dropout\nfrom keras.losses import mean_squared_error\nfrom keras.models import Model, clone_model, load_model\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport numpy as np\n\nclass RandomAgent(object):\n\n    def __init__(self, color=1):\n        self.color = color\n\n    def predict(self, board_layer):\n        return np.random.randint(-5, 5) / 5\n\n    def select_move(self, board):\n        moves = [x for x in board.generate_legal_moves()]\n        return np.random.choice(moves)\n    \nclass HumanAgent(object):\n    def predict(self, board):\n        move_inp = input()\n        move = chess.Move.from_uci(move_inp)\n        return move.uci()\n\n\nclass GreedyAgent(object):\n\n    def __init__(self, color=-1):\n        self.color = color\n\n    def predict(self, layer_board, noise=True):\n        layer_board1 = layer_board[0, :, :, :]\n        pawns = 1 * np.sum(layer_board1[0, :, :])\n        rooks = 5 * np.sum(layer_board1[1, :, :])\n        minor = 3 * np.sum(layer_board1[2:4, :, :])\n        queen = 9 * np.sum(layer_board1[4, :, :])\n\n        maxscore = 40\n        material = pawns + rooks + minor + queen\n        board_value = self.color * material / maxscore\n        if noise:\n            added_noise = np.random.randn() / 1e3\n        return board_value + added_noise\n\n\nclass Agent(object):\n\n    def __init__(self, lr=0.003, network='big'):\n        self.optimizer = RMSprop(lr=lr)\n        self.model = Model()\n        self.proportional_error = False\n        if network == 'simple':\n            self.init_simple_network()\n        elif network == 'super_simple':\n            self.init_super_simple_network()\n        elif network == 'alt':\n            self.init_altnet()\n        elif network == 'big':\n            self.init_bignet()\n        else:\n            self.init_network()\n\n    def fix_model(self):\n        \"\"\"\n        The fixed model is the model used for bootstrapping\n        Returns:\n        \"\"\"\n\n        self.fixed_model = clone_model(self.model)\n        self.fixed_model.compile(optimizer=self.optimizer, loss='mse', metrics=['mae'])\n        self.fixed_model.set_weights(self.model.get_weights())\n\n    def init_network(self):\n        layer_state = Input(shape=(8, 8, 8), name='state')\n\n        openfile = Conv2D(3, (8, 1), padding='valid', activation='relu', name='fileconv')(layer_state)  # 3,8,1\n        openrank = Conv2D(3, (1, 8), padding='valid', activation='relu', name='rankconv')(layer_state)  # 3,1,8\n        quarters = Conv2D(3, (4, 4), padding='valid', activation='relu', name='quarterconv', strides=(4, 4))(\n            layer_state)  # 3,2,2\n        large = Conv2D(8, (6, 6), padding='valid', activation='relu', name='largeconv')(layer_state)  # 8,2,2\n\n        board1 = Conv2D(16, (3, 3), padding='valid', activation='relu', name='board1')(layer_state)  # 16,6,6\n        board2 = Conv2D(20, (3, 3), padding='valid', activation='relu', name='board2')(board1)  # 20,4,4\n        board3 = Conv2D(24, (3, 3), padding='valid', activation='relu', name='board3')(board2)  # 24,2,2\n\n        flat_file = Flatten()(openfile)\n        flat_rank = Flatten()(openrank)\n        flat_quarters = Flatten()(quarters)\n        flat_large = Flatten()(large)\n\n        flat_board = Flatten()(board1)\n        flat_board3 = Flatten()(board3)\n\n        dense1 = Concatenate(name='dense_bass')(\n            [flat_file, flat_rank, flat_quarters, flat_large, flat_board, flat_board3])\n        dropout1 = Dropout(rate=0.1)(dense1)\n        dense2 = Dense(128, activation='sigmoid')(dropout1)\n        dense3 = Dense(64, activation='sigmoid')(dense2)\n        dropout3 = Dropout(rate=0.1)(dense3, training=True)\n        dense4 = Dense(32, activation='sigmoid')(dropout3)\n        dropout4 = Dropout(rate=0.1)(dense4, training=True)\n\n        value_head = Dense(1)(dropout4)\n        self.model = Model(inputs=layer_state,\n                           outputs=[value_head])\n        self.model.compile(optimizer=self.optimizer,\n                           loss=[mean_squared_error]\n                           )\n\n    def init_simple_network(self):\n\n        layer_state = Input(shape=(8, 8, 8), name='state')\n        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n        conv2 = Conv2D(6, (3, 3), activation='sigmoid')(conv1)\n        conv3 = Conv2D(4, (3, 3), activation='sigmoid')(conv2)\n        flat4 = Flatten()(conv3)\n        dense5 = Dense(24, activation='sigmoid')(flat4)\n        dense6 = Dense(8, activation='sigmoid')(dense5)\n        value_head = Dense(1)(dense6)\n\n        self.model = Model(inputs=layer_state,\n                           outputs=value_head)\n        self.model.compile(optimizer=self.optimizer,\n                           loss=mean_squared_error\n                           )\n\n    def init_super_simple_network(self):\n        layer_state = Input(shape=(8, 8, 8), name='state')\n        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n        flat4 = Flatten()(conv1)\n        dense5 = Dense(10, activation='sigmoid')(flat4)\n        value_head = Dense(1)(dense5)\n\n        self.model = Model(inputs=layer_state,\n                           outputs=value_head)\n        self.model.compile(optimizer=self.optimizer,\n                           loss=mean_squared_error\n                           )\n\n    def init_altnet(self):\n        layer_state = Input(shape=(8, 8, 8), name='state')\n        conv1 = Conv2D(6, (1, 1), activation='sigmoid')(layer_state)\n        flat2 = Flatten()(conv1)\n        dense3 = Dense(128, activation='sigmoid')(flat2)\n\n        value_head = Dense(1)(dense3)\n\n        self.model = Model(inputs=layer_state,\n                           outputs=value_head)\n        self.model.compile(optimizer=self.optimizer,\n                           loss=mean_squared_error\n                           )\n\n    def init_bignet(self):\n        layer_state = Input(shape=(8, 8, 8), name='state')\n        conv_xs = Conv2D(4, (1, 1), activation='relu')(layer_state)\n        conv_s = Conv2D(8, (2, 2), strides=(1, 1), activation='relu')(layer_state)\n        conv_m = Conv2D(12, (3, 3), strides=(2, 2), activation='relu')(layer_state)\n        conv_l = Conv2D(16, (4, 4), strides=(2, 2), activation='relu')(layer_state)\n        conv_xl = Conv2D(20, (8, 8), activation='relu')(layer_state)\n        conv_rank = Conv2D(3, (1, 8), activation='relu')(layer_state)\n        conv_file = Conv2D(3, (8, 1), activation='relu')(layer_state)\n\n        f_xs = Flatten()(conv_xs)\n        f_s = Flatten()(conv_s)\n        f_m = Flatten()(conv_m)\n        f_l = Flatten()(conv_l)\n        f_xl = Flatten()(conv_xl)\n        f_r = Flatten()(conv_rank)\n        f_f = Flatten()(conv_file)\n\n        dense1 = Concatenate(name='dense_bass')([f_xs, f_s, f_m, f_l, f_xl, f_r, f_f])\n        dense2 = Dense(256, activation='sigmoid')(dense1)\n        dense3 = Dense(128, activation='sigmoid')(dense2)\n        dense4 = Dense(56, activation='sigmoid')(dense3)\n        dense5 = Dense(64, activation='sigmoid')(dense4)\n        dense6 = Dense(32, activation='sigmoid')(dense5)\n\n        value_head = Dense(1)(dense6)\n\n        self.model = Model(inputs=layer_state,\n                           outputs=value_head)\n        self.model.compile(optimizer=self.optimizer,\n                           loss=mean_squared_error\n                           )\n\n    def predict_distribution(self, states, batch_size=256):\n        \"\"\"\n        :param states: list of distinct states\n        :param n:  each state is predicted n times\n        :return:\n        \"\"\"\n        predictions_per_state = int(batch_size / len(states))\n        state_batch = []\n        for state in states:\n            state_batch = state_batch + [state for x in range(predictions_per_state)]\n\n        state_batch = np.stack(state_batch, axis=0)\n        predictions = self.model.predict(state_batch)\n        predictions = predictions.reshape(len(states), predictions_per_state)\n        mean_pred = np.mean(predictions, axis=1)\n        std_pred = np.std(predictions, axis=1)\n        upper_bound = mean_pred + 2 * std_pred\n\n        return mean_pred, std_pred, upper_bound\n\n    def predict(self, board_layer):\n        return self.model.predict(board_layer)\n\n    def TD_update(self, states, rewards, sucstates, episode_active, gamma=0.9):\n        \"\"\"\n        Update the SARSA-network using samples from the minibatch\n        Args:\n            minibatch: list\n                The minibatch contains the states, moves, rewards and new states.\n\n        Returns:\n            td_errors: np.array\n                array of temporal difference errors\n\n        \"\"\"\n        suc_state_values = self.fixed_model.predict(sucstates)\n        V_target = np.array(rewards) + np.array(episode_active) * gamma * np.squeeze(suc_state_values)\n        # Perform a step of minibatch Gradient Descent.\n        self.model.fit(x=states, y=V_target, epochs=1, verbose=0)\n\n        V_state = self.model.predict(states)  # the expected future returns\n        td_errors = V_target - np.squeeze(V_state)\n\n        return td_errors\n\n    def MC_update(self, states, returns):\n        \"\"\"\n        Update network using a monte carlo playout\n        Args:\n            states: starting states\n            returns: discounted future rewards\n\n        Returns:\n            td_errors: np.array\n                array of temporal difference errors\n        \"\"\"\n        self.model.fit(x=states, y=returns, epochs=0, verbose=0)\n        V_state = np.squeeze(self.model.predict(states))\n        td_errors = returns - V_state\n\n        return td_errors","metadata":{"execution":{"iopub.status.busy":"2023-10-15T08:49:10.925446Z","iopub.execute_input":"2023-10-15T08:49:10.925851Z","iopub.status.idle":"2023-10-15T08:49:10.961484Z","shell.execute_reply.started":"2023-10-15T08:49:10.925822Z","shell.execute_reply":"2023-10-15T08:49:10.960477Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Board and Chess game environment","metadata":{}},{"cell_type":"code","source":"import chess\nimport numpy as np\n\nmapper = {}\nmapper[\"p\"] = 0\nmapper[\"r\"] = 1\nmapper[\"n\"] = 2\nmapper[\"b\"] = 3\nmapper[\"q\"] = 4\nmapper[\"k\"] = 5\nmapper[\"P\"] = 0\nmapper[\"R\"] = 1\nmapper[\"N\"] = 2\nmapper[\"B\"] = 3\nmapper[\"Q\"] = 4\nmapper[\"K\"] = 5\n\n\nclass Board(object):\n\n    def __init__(self, opposing_agent, FEN=None, capture_reward_factor=0.01):\n        \"\"\"\n        Chess Board Environment\n        Args:\n            FEN: str\n                Starting FEN notation, if None then start in the default chess position\n            capture_reward_factor: float [0,inf]\n                reward for capturing a piece. Multiply material gain by this number. 0 for normal chess.\n        \"\"\"\n        self.FEN = FEN\n        self.capture_reward_factor = capture_reward_factor\n        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n        self.layer_board = np.zeros(shape=(8, 8, 8))\n        self.init_layer_board()\n        self.opposing_agent = opposing_agent\n\n    def init_layer_board(self):\n        \"\"\"\n        Initalize the numerical representation of the environment\n        Returns:\n\n        \"\"\"\n        self.layer_board = np.zeros(shape=(8, 8, 8))\n        for i in range(64):\n            row = i // 8\n            col = i % 8\n            piece = self.board.piece_at(i)\n            if piece == None:\n                continue\n            elif piece.symbol().isupper():\n                sign = 1\n            else:\n                sign = -1\n            layer = mapper[piece.symbol()]\n            self.layer_board[layer, row, col] = sign\n            self.layer_board[6, :, :] = 1 / self.board.fullmove_number\n        if self.board.turn:\n            self.layer_board[6, 0, :] = 1\n        else:\n            self.layer_board[6, 0, :] = -1\n        self.layer_board[7, :, :] = 1\n\n    def update_layer_board(self, move=None):\n        self._prev_layer_board = self.layer_board.copy()\n        self.init_layer_board()\n\n    def pop_layer_board(self):\n        self.layer_board = self._prev_layer_board.copy()\n        self._prev_layer_board = None\n\n    def step(self, action, test=True):\n        \"\"\"\n        Run a step\n        Args:\n            action: python chess move\n        Returns:\n            epsiode end: Boolean\n                Whether the episode has ended\n            reward: float\n                Difference in material value after the move\n        \"\"\"\n        piece_balance_before = self.get_material_value()\n        self.board.push(action)\n        self.update_layer_board(action)\n        piece_balance_after = self.get_material_value()\n        auxiliary_reward = (piece_balance_after - piece_balance_before) * self.capture_reward_factor\n        result = self.board.result()\n        if result == \"*\":\n            reward = 0\n            episode_end = False\n        elif result == \"1-0\":\n            reward = 1\n            episode_end = True\n        elif result == \"0-1\":\n            reward = -1\n            episode_end = True\n        elif result == \"1/2-1/2\":\n            reward = 0\n            episode_end = True\n        reward += auxiliary_reward\n\n        return episode_end, reward\n\n    def get_random_action(self):\n        \"\"\"\n        Sample a random action\n        Returns: move\n            A legal python chess move.\n\n        \"\"\"\n        legal_moves = [x for x in self.board.generate_legal_moves()]\n        legal_moves = np.random.choice(legal_moves)\n        return legal_moves\n\n    def project_legal_moves(self):\n        \"\"\"\n        Create a mask of legal actions\n        Returns: np.ndarray with shape (64,64)\n        \"\"\"\n        self.action_space = np.zeros(shape=(64, 64))\n        moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n        for move in moves:\n            self.action_space[move[0], move[1]] = 1\n        return self.action_space\n\n    def get_material_value(self):\n        \"\"\"\n        Sums up the material balance using Reinfield values\n        Returns: The material balance on the board\n        \"\"\"\n        pawns = 1 * np.sum(self.layer_board[0, :, :])\n        rooks = 5 * np.sum(self.layer_board[1, :, :])\n        minor = 3 * np.sum(self.layer_board[2:4, :, :])\n        queen = 9 * np.sum(self.layer_board[4, :, :])\n        return pawns + rooks + minor + queen\n\n    def reset(self):\n        \"\"\"\n        Reset the environment\n        Returns:\n\n        \"\"\"\n        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n        self.init_layer_board()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T08:53:47.369993Z","iopub.execute_input":"2023-10-15T08:53:47.370364Z","iopub.status.idle":"2023-10-15T08:53:47.387105Z","shell.execute_reply.started":"2023-10-15T08:53:47.370336Z","shell.execute_reply":"2023-10-15T08:53:47.386099Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Monte Carlo Tree Search implementation for game tree simulation ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\ndef softmax(x, temperature=1):\n    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n\n\nclass Node(object):\n\n    def __init__(self, board=None, parent=None, gamma=0.9):\n        \"\"\"\n        Game Node for Monte Carlo Tree Search\n        Args:\n            board: the chess board\n            parent: the parent node\n            gamma: the discount factor\n        \"\"\"\n        self.children = {}  # Child nodes\n        self.board = board  # Chess board\n        self.parent = parent\n        self.values = []  # reward + Returns\n        self.gamma = gamma\n        self.starting_value = 0\n\n    def update_child(self, move, Returns):\n        \"\"\"\n        Update a child with a simulation result\n        Args:\n            move: The move that leads to the child\n            Returns: the reward of the move and subsequent returns\n\n        Returns:\n\n        \"\"\"\n        child = self.children[move]\n        child.values.append(Returns)\n\n    def update(self, Returns=None):\n        \"\"\"\n        Update a node with observed Returns\n        Args:\n            Returns: Future returns\n\n        Returns:\n\n        \"\"\"\n        if Returns:\n            self.values.append(Returns)\n\n    def select(self, color=1):\n        \"\"\"\n        Use Thompson sampling to select the best child node\n        Args:\n            color: Whether to select for white or black\n\n        Returns:\n            (node, move)\n            node: the selected node\n            move: the selected move\n        \"\"\"\n        assert color == 1 or color == -1, \"color has to be white (1) or black (-1)\"\n        if self.children:\n            max_sample = np.random.choice(color * np.array(self.values))\n            max_move = None\n            for move, child in self.children.items():\n                child_sample = np.random.choice(color * np.array(child.values))\n                if child_sample > max_sample:\n                    max_sample = child_sample\n                    max_move = move\n            if max_move:\n                return self.children[max_move], max_move\n            else:\n                return self, None\n        else:\n            return self, None\n\n    def simulate(self, model, env, depth=0, max_depth=4, random=False, temperature=1):\n        \"\"\"\n        Recursive Monte Carlo Playout\n        Args:\n            model: The model used for bootstrap estimation\n            env: the chess environment\n            depth: The recursion depth\n            max_depth: How deep to search\n            temperature: softmax temperature\n\n        Returns:\n            Playout result.\n        \"\"\"\n        board_in = env.board.fen()\n        if env.board.turn and random:\n            move = np.random.choice([x for x in env.board.generate_legal_moves()])\n        else:\n            successor_values = []\n            for move in env.board.generate_legal_moves():\n                episode_end, reward = env.step(move)\n                result = env.board.result()\n\n                if (result == \"1-0\" and env.board.turn) or (\n                        result == \"0-1\" and not env.board.turn):\n                    env.board.pop()\n                    env.init_layer_board()\n                    break\n                else:\n                    if env.board.turn:\n                        sucval = reward + self.gamma * np.squeeze(\n                            model.predict(np.expand_dims(env.layer_board, axis=0)))\n                    else:\n                        sucval = np.squeeze(env.opposing_agent.predict(np.expand_dims(env.layer_board, axis=0)))\n                    successor_values.append(sucval)\n                    env.board.pop()\n                    env.init_layer_board()\n\n            if not episode_end:\n                if env.board.turn:\n                    move_probas = softmax(np.array(successor_values), temperature=temperature)\n                    moves = [x for x in env.board.generate_legal_moves()]\n                else:\n                    move_probas = np.zeros(len(successor_values))\n                    move_probas[np.argmax(successor_values)] = 1\n                    moves = [x for x in env.board.generate_legal_moves()]\n                if len(moves) == 1:\n                    move = moves[0]\n                else:\n                    move = np.random.choice(moves, p=np.squeeze(move_probas))\n\n        episode_end, reward = env.step(move)\n\n        if episode_end:\n            Returns = reward\n        elif depth >= max_depth:  # Bootstrap the Monte Carlo Playout\n            Returns = reward + self.gamma * np.squeeze(model.predict(np.expand_dims(env.layer_board, axis=0)))\n        else:  # Recursively continue\n            Returns = reward + self.gamma * self.simulate(model, env, depth=depth + 1,temperature=temperature)\n\n        env.board.pop()\n        env.init_layer_board()\n\n        board_out = env.board.fen()\n        assert board_in == board_out\n\n        if depth == 0:\n            return Returns, move\n        else:\n            noise = np.random.randn() / 1e6\n            return Returns + noise","metadata":{"execution":{"iopub.status.busy":"2023-10-15T07:31:10.688707Z","iopub.execute_input":"2023-10-15T07:31:10.689022Z","iopub.status.idle":"2023-10-15T07:31:10.704823Z","shell.execute_reply.started":"2023-10-15T07:31:10.688997Z","shell.execute_reply":"2023-10-15T07:31:10.703771Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Game simulation and Neural network batch creation, preprocessing with current, next board state and corresponding rewards for each move","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport math\nimport gc\n\n\ndef softmax(x, temperature=1):\n    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\n\nclass TD_search(object):\n\n    def __init__(self, env, agent, gamma=0.9, search_time=1, memsize=2000, batch_size=256, temperature=1):\n        \"\"\"\n        Chess algorithm that combines bootstrapped monte carlo tree search with Q Learning\n        Args:\n            env: RLC chess environment\n            agent: RLC chess agent\n            gamma: discount factor\n            search_time: maximum time spent doing tree search\n            memsize: Amount of training samples to keep in-memory\n            batch_size: Size of the training batches\n            temperature: softmax temperature for mcts\n        \"\"\"\n        self.env = env\n        self.agent = agent\n        self.tree = Node(self.env)\n        self.gamma = gamma\n        self.memsize = memsize\n        self.batch_size = batch_size\n        self.temperature = temperature\n        self.reward_trace = []  # Keeps track of the rewards\n        self.piece_balance_trace = []  # Keep track of the material value on the board\n        self.ready = False  # Whether to start training\n        self.search_time = search_time\n        self.min_sim_count = 10\n\n        self.mem_state = np.zeros(shape=(1, 8, 8, 8))\n        self.mem_sucstate = np.zeros(shape=(1, 8, 8, 8))\n        self.mem_reward = np.zeros(shape=(1))\n        self.mem_error = np.zeros(shape=(1))\n        self.mem_episode_active = np.ones(shape=(1))\n\n    def learn(self, iters=40, c=5, timelimit_seconds=3600, maxiter=80):\n        \"\"\"\n        Start Reinforcement Learning Algorithm\n        Args:\n            iters: maximum amount of iterations to train\n            c: model update rate (once every C games)\n            timelimit_seconds: maximum training time\n            maxiter: Maximum duration of a game, in halfmoves\n        Returns:\n\n        \"\"\"\n        starttime = time.time()\n        for k in range(iters):\n            self.env.reset()\n            if k % c == 0:\n                self.agent.fix_model()\n                print(\"iter\", k)\n            if k > c:\n                self.ready = True\n            self.play_game(k, maxiter=maxiter)\n            if starttime + timelimit_seconds < time.time():\n                break\n        return self.env.board\n\n    def play_game(self, k, maxiter=80):\n        \"\"\"\n        Play a chess game and learn from it\n        Args:\n            k: the play iteration number\n            maxiter: maximum duration of the game (halfmoves)\n\n        Returns:\n            board: Chess environment on terminal state\n        \"\"\"\n        episode_end = False\n        turncount = 0\n        tree = Node(self.env.board, gamma=self.gamma)  # Initialize the game tree\n\n        # Play a game of chess\n        while not episode_end:\n            state = np.expand_dims(self.env.layer_board.copy(), axis=0)\n            state_value = self.agent.predict(state)\n\n            # White's turn involves tree-search\n            if self.env.board.turn:\n\n                # Do a Monte Carlo Tree Search after game iteration k\n                start_mcts_after = -1\n                if k > start_mcts_after:\n                    tree = self.mcts(tree)\n                    # Step the best move\n                    max_move = None\n                    max_value = np.NINF\n                    for move, child in tree.children.items():\n                        sampled_value = np.mean(child.values)\n                        if sampled_value > max_value:\n                            max_value = sampled_value\n                            max_move = move\n                else:\n                    max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n\n            # Black's turn is myopic\n            else:\n                max_move = None\n                max_value = np.NINF\n                for move in self.env.board.generate_legal_moves():\n                    self.env.step(move)\n                    if self.env.board.result() == \"0-1\":\n                        max_move = move\n                        self.env.board.pop()\n                        self.env.init_layer_board()\n                        break\n                    successor_state_value_opponent = self.env.opposing_agent.predict(\n                        np.expand_dims(self.env.layer_board, axis=0))\n                    if successor_state_value_opponent > max_value:\n                        max_move = move\n                        max_value = successor_state_value_opponent\n\n                    self.env.board.pop()\n                    self.env.init_layer_board()\n\n            if not (self.env.board.turn and max_move not in tree.children.keys()) or not k > start_mcts_after:\n                tree.children[max_move] = Node(gamma=0.9, parent=tree)\n\n            episode_end, reward = self.env.step(max_move)\n\n            tree = tree.children[max_move]\n            tree.parent = None\n            gc.collect()\n\n            sucstate = np.expand_dims(self.env.layer_board, axis=0)\n            new_state_value = self.agent.predict(sucstate)\n\n            error = reward + self.gamma * new_state_value - state_value\n            error = np.float(np.squeeze(error))\n\n            turncount += 1\n            if turncount > maxiter and not episode_end:\n                episode_end = True\n\n            episode_active = 0 if episode_end else 1\n\n            # construct training sample state, prediction, error\n            self.mem_state = np.append(self.mem_state, state, axis=0)\n            self.mem_reward = np.append(self.mem_reward, reward)\n            self.mem_sucstate = np.append(self.mem_sucstate, sucstate, axis=0)\n            self.mem_error = np.append(self.mem_error, error)\n            self.reward_trace = np.append(self.reward_trace, reward)\n            self.mem_episode_active = np.append(self.mem_episode_active, episode_active)\n\n            if self.mem_state.shape[0] > self.memsize:\n                self.mem_state = self.mem_state[1:]\n                self.mem_reward = self.mem_reward[1:]\n                self.mem_sucstate = self.mem_sucstate[1:]\n                self.mem_error = self.mem_error[1:]\n                self.mem_episode_active = self.mem_episode_active[1:]\n                gc.collect()\n\n            if turncount % 10 == 0:\n                self.update_agent()\n\n        piece_balance = self.env.get_material_value()\n        self.piece_balance_trace.append(piece_balance)\n        print(\"game ended with result\", reward, \"and material balance\", piece_balance, \"in\", turncount, \"halfmoves\")\n\n        return self.env.board\n\n    def update_agent(self):\n        \"\"\"\n        Update the Agent with TD learning\n        Returns:\n            None\n        \"\"\"\n        if self.ready:\n            choice_indices, states, rewards, sucstates, episode_active = self.get_minibatch()\n            td_errors = self.agent.TD_update(states, rewards, sucstates, episode_active, gamma=self.gamma)\n            self.mem_error[choice_indices.tolist()] = td_errors\n\n    def get_minibatch(self, prioritized=True):\n        \"\"\"\n        Get a mini batch of experience\n        Args:\n            prioritized:\n\n        Returns:\n\n        \"\"\"\n        if prioritized:\n            sampling_priorities = np.abs(self.mem_error) + 1e-9\n        else:\n            sampling_priorities = np.ones(shape=self.mem_error.shape)\n        sampling_probs = sampling_priorities / np.sum(sampling_priorities)\n        sample_indices = [x for x in range(self.mem_state.shape[0])]\n        choice_indices = np.random.choice(sample_indices,\n                                          min(self.mem_state.shape[0],\n                                              self.batch_size),\n                                          p=np.squeeze(sampling_probs),\n                                          replace=False\n                                          )\n        states = self.mem_state[choice_indices]\n        rewards = self.mem_reward[choice_indices]\n        sucstates = self.mem_sucstate[choice_indices]\n        episode_active = self.mem_episode_active[choice_indices]\n\n        return choice_indices, states, rewards, sucstates, episode_active\n\n    def mcts(self, node):\n        \"\"\"\n        Run Monte Carlo Tree Search\n        Args:\n            node: A game state node object\n\n        Returns:\n            the node with playout sims\n\n        \"\"\"\n\n        starttime = time.time()\n        sim_count = 0\n        board_in = self.env.board.fen()\n\n        # First make a prediction for each child state\n        for move in self.env.board.generate_legal_moves():\n            if move not in node.children.keys():\n                node.children[move] = Node(self.env.board, parent=node)\n\n            episode_end, reward = self.env.step(move)\n\n            if episode_end:\n                successor_state_value = 0\n            else:\n                successor_state_value = np.squeeze(\n                    self.agent.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n                )\n\n            child_value = reward + self.gamma * successor_state_value\n\n            node.update_child(move, child_value)\n            self.env.board.pop()\n            self.env.init_layer_board()\n        if not node.values:\n            node.values = [0]\n\n        while starttime + self.search_time > time.time() or sim_count < self.min_sim_count:\n            depth = 0\n            color = 1\n            node_rewards = []\n\n            # Select the best node from where to start MCTS\n            while node.children:\n                node, move = node.select(color=color)\n                if not move:\n                    # No move means that the node selects itself, not a child node.\n                    break\n                else:\n                    depth += 1\n                    color = color * -1  # switch color\n                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n                    node_rewards.append(reward)\n                    # Check best node is terminal\n\n                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n                        self.env.board.pop()\n                        self.env.init_layer_board()\n                        node.update(1)\n                        node = node.parent\n                        return node\n                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n                        while node.parent:\n                            self.env.board.pop()\n                            self.env.init_layer_board()\n                            node = node.parent\n                        break\n                    else:\n                        continue\n\n            # Expand the game tree with a simulation\n            Returns, move = node.simulate(self.agent.fixed_model,\n                                          self.env,\n                                          temperature=self.temperature,\n                                          depth=0)\n            self.env.init_layer_board()\n\n            if move not in node.children.keys():\n                node.children[move] = Node(self.env.board, parent=node)\n\n            node.update_child(move, Returns)\n\n            # Return to root node and backpropagate Returns\n            while node.parent:\n                latest_reward = node_rewards.pop(-1)\n                Returns = latest_reward + self.gamma * Returns\n                node.update(Returns)\n                node = node.parent\n\n                self.env.board.pop()\n                self.env.init_layer_board()\n            sim_count += 1\n\n        board_out = self.env.board.fen()\n        assert board_in == board_out\n\n        return node","metadata":{"execution":{"iopub.status.busy":"2023-10-15T07:31:12.113481Z","iopub.execute_input":"2023-10-15T07:31:12.113844Z","iopub.status.idle":"2023-10-15T07:31:12.143528Z","shell.execute_reply.started":"2023-10-15T07:31:12.113815Z","shell.execute_reply":"2023-10-15T07:31:12.142698Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Reinforcement Learning agent neural network training","metadata":{}},{"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\n\nimport chess\nfrom chess.pgn import Game\n\nopponent = GreedyAgent()\nenv = Board(opponent, FEN=None)\nplayer = Agent(lr=0.01, network='big')\nlearner = TD_search(env, player, gamma=0.8, search_time=2)\nnode = Node(learner.env.board, gamma=learner.gamma)\nplayer.model.summary()\n\nlearner.learn(iters=1, timelimit_seconds=60)\n\nreward_smooth = pd.DataFrame(learner.reward_trace)\nreward_smooth.rolling(window=500, min_periods=0).mean().plot(figsize=(16, 9),\n                                                             title='average performance over the last 3 episodes')\nplt.show()\n\nreward_smooth = pd.DataFrame(learner.piece_balance_trace)\nreward_smooth.rolling(window=100, min_periods=0).mean().plot(figsize=(16, 9),\n                                                             title='average piece balance over the last 3 episodes')\nplt.show()\n\npgn = Game.from_board(learner.env.board)\nwith open(\"rlc_pgn\", \"w\") as log:\n    log.write(str(pgn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learner.agent.model.save(\"my_model.keras\")","metadata":{"execution":{"iopub.status.busy":"2023-10-15T07:53:36.614057Z","iopub.execute_input":"2023-10-15T07:53:36.614423Z","iopub.status.idle":"2023-10-15T07:53:36.657016Z","shell.execute_reply.started":"2023-10-15T07:53:36.614394Z","shell.execute_reply":"2023-10-15T07:53:36.656088Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"ai_player.model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T08:00:49.278454Z","iopub.execute_input":"2023-10-15T08:00:49.279137Z","iopub.status.idle":"2023-10-15T08:00:49.321670Z","shell.execute_reply.started":"2023-10-15T08:00:49.279107Z","shell.execute_reply":"2023-10-15T08:00:49.320968Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n state (InputLayer)             [(None, 8, 8, 8)]    0           []                               \n                                                                                                  \n conv2d (Conv2D)                (None, 8, 8, 4)      36          ['state[0][0]']                  \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 7, 7, 8)      264         ['state[0][0]']                  \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 3, 3, 12)     876         ['state[0][0]']                  \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 3, 3, 16)     2064        ['state[0][0]']                  \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 1, 1, 20)     10260       ['state[0][0]']                  \n                                                                                                  \n conv2d_5 (Conv2D)              (None, 8, 1, 3)      195         ['state[0][0]']                  \n                                                                                                  \n conv2d_6 (Conv2D)              (None, 1, 8, 3)      195         ['state[0][0]']                  \n                                                                                                  \n flatten (Flatten)              (None, 256)          0           ['conv2d[0][0]']                 \n                                                                                                  \n flatten_1 (Flatten)            (None, 392)          0           ['conv2d_1[0][0]']               \n                                                                                                  \n flatten_2 (Flatten)            (None, 108)          0           ['conv2d_2[0][0]']               \n                                                                                                  \n flatten_3 (Flatten)            (None, 144)          0           ['conv2d_3[0][0]']               \n                                                                                                  \n flatten_4 (Flatten)            (None, 20)           0           ['conv2d_4[0][0]']               \n                                                                                                  \n flatten_5 (Flatten)            (None, 24)           0           ['conv2d_5[0][0]']               \n                                                                                                  \n flatten_6 (Flatten)            (None, 24)           0           ['conv2d_6[0][0]']               \n                                                                                                  \n dense_bass (Concatenate)       (None, 968)          0           ['flatten[0][0]',                \n                                                                  'flatten_1[0][0]',              \n                                                                  'flatten_2[0][0]',              \n                                                                  'flatten_3[0][0]',              \n                                                                  'flatten_4[0][0]',              \n                                                                  'flatten_5[0][0]',              \n                                                                  'flatten_6[0][0]']              \n                                                                                                  \n dense (Dense)                  (None, 256)          248064      ['dense_bass[0][0]']             \n                                                                                                  \n dense_1 (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n                                                                                                  \n dense_2 (Dense)                (None, 56)           7224        ['dense_1[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 64)           3648        ['dense_2[0][0]']                \n                                                                                                  \n dense_4 (Dense)                (None, 32)           2080        ['dense_3[0][0]']                \n                                                                                                  \n dense_5 (Dense)                (None, 1)            33          ['dense_4[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 307,835\nTrainable params: 307,835\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"class Player(object):\n    def play_move(self):\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:32:28.948993Z","iopub.execute_input":"2023-10-15T09:32:28.950041Z","iopub.status.idle":"2023-10-15T09:32:28.954792Z","shell.execute_reply.started":"2023-10-15T09:32:28.950003Z","shell.execute_reply":"2023-10-15T09:32:28.953826Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## Human Player","metadata":{}},{"cell_type":"code","source":"class HumanPlayer(Player):\n    def __init__(self, env, agent):\n        \"\"\"\n        env: Chess Board environment\n        agent: Human agent\n        \"\"\"\n        self.env = env\n        self.agent = agent\n    \n    def play_best_move(self):\n        return self.agent.predict(self.env.layer_board)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:34:03.904059Z","iopub.execute_input":"2023-10-15T09:34:03.904733Z","iopub.status.idle":"2023-10-15T09:34:03.910867Z","shell.execute_reply.started":"2023-10-15T09:34:03.904690Z","shell.execute_reply":"2023-10-15T09:34:03.909910Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## AI and RandomPlayer class","metadata":{}},{"cell_type":"code","source":"class AIandRandomPlayer(Player):\n    def __init__(self, env, agent, gamma):\n        \"\"\"\n        env: Chess Board environment\n        agent: Random agent/ AI agent trained with reinforcement learning based on temporal difference learning and\n        Monte Carlo Tree Search for game simulation\n        gamma: decay factor for successive rewards\n        \"\"\"\n        self.env = env\n        self.agent = agent\n        self.gamma = gamma\n    \n    def play_best_move(self):\n        max_move=None\n        max_child_value = None\n        for move in self.env.board.generate_legal_moves():\n            episode_end, reward = self.env.step(move)\n            if episode_end:\n                successor_state_value = 0\n            else:\n                successor_state_value = np.squeeze(\n                    self.agent.predict(np.expand_dims(self.env.layer_board, axis=0))\n                )\n\n            child_value = reward + self.gamma * successor_state_value\n            if max_child_value is None:\n                max_child_value = child_value\n                max_move = move\n            elif child_value > max_child_value:\n                max_child_value = child_value\n                max_move = move\n            \n            # Reset the board after predict the move's score\n            self.env.board.pop()\n            self.env.init_layer_board()\n        \n        return max_move.uci()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:43:28.737546Z","iopub.execute_input":"2023-10-15T09:43:28.737911Z","iopub.status.idle":"2023-10-15T09:43:28.745783Z","shell.execute_reply.started":"2023-10-15T09:43:28.737887Z","shell.execute_reply":"2023-10-15T09:43:28.744849Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"## Initialising game playing agents","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\n\nenv = Board(opposing_agent=None, FEN=None)\n\nai_agent = Agent(lr=0.01, network='big')\nai_agent.model = load_model('/kaggle/input/rlc-model-vigneshrc/RLC_model.h5')\nrandom_agent = RandomAgent()\nhuman_agent = HumanAgent()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:43:49.877439Z","iopub.execute_input":"2023-10-15T09:43:49.878427Z","iopub.status.idle":"2023-10-15T09:43:50.197592Z","shell.execute_reply.started":"2023-10-15T09:43:49.878385Z","shell.execute_reply":"2023-10-15T09:43:50.196643Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"ai_player = AIandRandomPlayer(env=env, agent=ai_agent, gamma=learner.gamma)\nrandom_player = AIandRandomPlayer(env=env, agent=random_agent, gamma=learner.gamma)\nhuman_player = HumanPlayer(env=env, agent=human_agent)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:43:52.161527Z","iopub.execute_input":"2023-10-15T09:43:52.162175Z","iopub.status.idle":"2023-10-15T09:43:52.166931Z","shell.execute_reply.started":"2023-10-15T09:43:52.162144Z","shell.execute_reply":"2023-10-15T09:43:52.166045Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"## Game play visualization","metadata":{}},{"cell_type":"code","source":"import time\nfrom IPython.display import display, HTML, clear_output\n\ndef who(player):\n    return \"White\" if player == chess.WHITE else \"Black\"\ndef display_board(board, use_svg):\n    if use_svg:\n        return board._repr_svg_()\n    else:\n        return \"<pre>\" + str(board) + \"</pre>\"\n    \ndef play_game_visual(player1, player2, visual=\"svg\", pause=0.1):\n    \"\"\"\n    playerN1, player2: functions that takes board, return uci move\n    visual: \"simple\" | \"svg\" | None\n    \"\"\"\n    use_svg = (visual == \"svg\")\n    # board = chess.Board()\n    board = env.board\n    board_stop = display_board(env.board, use_svg)\n    html = \"%s\" % (board_stop)\n    display(HTML(html))\n    try:\n        #board = env.board\n        while not board.is_game_over(claim_draw=True):\n            if board.turn == chess.WHITE:\n                uci = player1.play_best_move()\n            else:\n                uci = player2.play_best_move()\n            name = who(board.turn)\n            board.push_uci(uci)\n            board_stop = display_board(board, use_svg)\n            html = \"<h1>Move %s %s, Play '%s':</h1><br/>%s\" % (\n                       len(board.move_stack), name, uci, board_stop)\n            if visual is not None:\n                if visual == \"svg\":\n                    clear_output(wait=True)\n                display(HTML(html))\n                if visual == \"svg\":\n                    time.sleep(pause)\n    except KeyboardInterrupt:\n        msg = \"Game interrupted!\"\n        return (None, msg, board)\n    result = None\n    if board.is_checkmate():\n        msg = \"checkmate: \" + who(not board.turn) + \" wins!\"\n        result = not board.turn\n    elif board.is_stalemate():\n        msg = \"draw: stalemate\"\n    elif board.is_fivefold_repetition():\n        msg = \"draw: 5-fold repetition\"\n    elif board.is_insufficient_material():\n        msg = \"draw: insufficient material\"\n    elif board.can_claim_draw():\n        msg = \"draw: claim\"\n    if visual is not None:\n        print(msg)\n    return (result, msg, board)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:44:28.820970Z","iopub.execute_input":"2023-10-15T09:44:28.821312Z","iopub.status.idle":"2023-10-15T09:44:28.830936Z","shell.execute_reply.started":"2023-10-15T09:44:28.821284Z","shell.execute_reply":"2023-10-15T09:44:28.830008Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"play_game_visual(ai_player, random_player, pause=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:44:53.937897Z","iopub.execute_input":"2023-10-15T09:44:53.938268Z","iopub.status.idle":"2023-10-15T09:45:42.499768Z","shell.execute_reply.started":"2023-10-15T09:44:53.938238Z","shell.execute_reply":"2023-10-15T09:45:42.498806Z"},"trusted":true},"execution_count":83,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h1>Move 38 Black, Play 'f7g8':</h1><br/><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r . . . . N k .\n. p . . p . . .\np . p . . . . .\n. . . . P . . .\n. . P . . . . P\nP . . . P . . .\n. P . . B P P .\nR . B . K . . R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light lastmove f7\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light lastmove g8\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 150)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(240, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(285, 15)\" /></svg>"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 17ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 18ms/step\n1/1 [==============================] - 0s 19ms/step\n","output_type":"stream"},{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"(None,\n 'Game interrupted!',\n Board('r4Nk1/1p2p3/p1p5/4P3/2P4P/P3P2R/1P2BPP1/R1B1K3 b Q - 3 20'))"},"metadata":{}}]}]}